{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsNc6CvUPEm9v9apezmE+g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hananather/DCP/blob/main/Function_Approximation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Need for Function approximation\n",
        "\n",
        "- So far, we have been assuming that all the value functions (state value or action value) can be represented as a table\n",
        "\n",
        "- this is only possible if the state and action space are small\n",
        "\n",
        "- Many real word problems have large state and action space which tabular representation are insufficient.\n",
        "\n",
        "- Example: Backgammon has $10^{20}$ states and Computer Go has $10^{170}$ states\n"
      ],
      "metadata": {
        "id": "rB6gf4wxQJUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to approximate $v_{\\pi}$ from experience generated from a policy $\\pi$.\n",
        "We write \n",
        "$$\n",
        "\\hat{v}(s,\\boldsymbol{w}) \\approx v_{\\pi}(s)\n",
        "$$\n",
        "for the approximate value of state $s$.\n",
        "\n",
        "Function approximation methods expect to receive examples of the desired input-output behaviour of the function they are trying to approximate. We view each update as a conventional training example. \n",
        "\n",
        "In GPI we seek to learn $q^{\\pi}$ while $\\pi$ changes. Even"
      ],
      "metadata": {
        "id": "q_wKBcUE3a68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An explicit objective for prediction was not necessary because the learned values would come to equal the the true value function exactly. Moreover, the learned values at each state were decoupled. \n",
        "\n",
        "We have for more states than weights, so by making one state's estimate more accurate we invariably means making others less accurate. \n",
        "\n",
        "We specificy a state distributin $\\sum_s \\mu(s) = 1$, representing how much we care about the error in each state $s$.\n",
        "\n",
        "\n",
        "\n",
        "The Mean Squared Value Error:\n",
        "$$\n",
        "\\bar{VE}(w) = \\sum_{s \\in S}\\mu(s)[v^{\\pi}(s) - \\hat{v}(s,w) ]^2\n",
        "$$"
      ],
      "metadata": {
        "id": "0WyexSoy6qAy"
      }
    }
  ]
}