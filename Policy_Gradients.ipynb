{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5P8mrLg7U9LmnFLId+hGq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hananather/DCP/blob/main/Policy_Gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's understand what the policy function is. In a policy gradient algorithm, the policy, often denoted as $\\pi(a|s,\\theta)$, is a function that maps states to actions, **based oo current parameters**. \n",
        "\n",
        "1. Initialize the Policy Parameters: Initialize the parameters $\\theta$ of the policy function. The specific initialization can vary; a common choice is to initialize the parameters randomly. \n",
        "\n",
        "2. **Interact with Environment**: For each episode, the agent interacts with the environment by taking actions according to the current policy $\\pi(a|s,\\theta)$, and collects trajectories. Each trajectory is a sequence of states, actions, and rewards $(S_0, a_0, r_0, s_1, a_1, r_1, \\dots s_T)$\n",
        "\n",
        "3. **Calculate Returns:** For each time step in the trajectory, calculate the return $G_t$. This is the cumulative discount reward from that time step to the end of the episode. \n",
        "\n",
        "4. **Update Policy Parameters:** Update the policy parameters $\\theta$ using gradient ascent, in the direction that maximizes the expected return. Specifically, the update rule is : \n",
        "$$\n",
        "\\theta = \\theta + \\alpha \\nabla log \\pi(a|s,\\theta) G_t\n",
        "$$\n",
        "\n",
        "The gradient $\\nabla log \\pi (a|s, \\theta) G_t$ can be thought of as scaling the log probability of the action by the return. If the return $G_t$ is postive, this means, the action led to a good outcome, so we want to increase its log probability. If the return $G_t$ is negative, this means, the action led to a bad outcome, so we want decrease its log probability.\n",
        "\n",
        "\n",
        "**Why do we use $log \\pi(a|s,\\theta) G_t$ ?**\n",
        "Taking the derivative of the log policy, $\\nabla_{\\theta} log \\pi(a|s,\\theta)$, is equivalent to taking the derivative of the policy, with respect to the policy parameters $\\theta$, scaled by the policy itself. \n",
        "\n",
        "The return G_t from a state-action trajectory represents how good that trajectory was: a higher return means the trajectory was better. By multiplying the log policy derivative by the return, we are basically scaling the updates to the policy parameters based on the quality of the trajectory. If the return was postive\n",
        "\n",
        "I'm sure if I fully see how by simply increasing the value of $\\theta$ we are increasing the probability of taking that particular action in that state?\n",
        "Answer: The detials of how updating the parameters $\\theta$ influences the probabilities of actions depends on the specific form of the policy function $\\pi(a|s,\\theta)$. \n",
        "In policy gradient methods, the policy is typically parameterized as a differentiable function that outputs a probability distribution over actions for each state. A common choice for this is the softmax function in the case of discrete action spaces, or a Gaussian distribution in the case of continous action spaces. \n",
        "\n",
        "When a policy is parameterized by a neural network, the the parameter $\\theta$ represents the weights and biases of the network. **The policy becomes a function computed by the neural network that outputs a probability dsitribution over actions given a state.** The neural network takes the state as an input and prodc\n",
        "\n",
        "\n",
        "How exactly do we calulate the return $G_t$ in practice? This can only be computed after we have completed a complete episode right? \n",
        "\n",
        "I'm still unclear on \"update the the policy parameters $\\theta$ using gradient ascent, in the direction that maximizes expected return\", in this case is the \"expected return G_t? \n",
        "\n"
      ],
      "metadata": {
        "id": "D6s1XHQTkRIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I'm sure if I fully see how by simply increasing the value of $\\theta$ we are increasing the probability of taking that particular action in that state?\n",
        "\n",
        "Answer: The detials of how updating the parameters $\\theta$ influences the probabilities of actions depends on the specific form of the policy function $\\pi(a|s,\\theta)$. \n",
        "In policy gradient methods, the policy is typically parameterized as a differentiable function that outputs a probability distribution over actions for each state. A common choice for this is the softmax function in the case of discrete action spaces, or a Gaussian distribution in the case of continous action spaces. \n",
        "\n",
        "When a policy is parameterized by a neural network, the the parameter $\\theta$ represents the weights and biases of the network. **The policy becomes a function computed by the neural network that outputs a probability dsitribution over actions given a state.** The neural network takes the state as an input and produces as output the parameters of this distrubution. \n",
        "\n",
        "1. Forward Pass: Given a state, perform a forward pass through the network to compute the output probabilities for each action. \n",
        "\n",
        "2. Next, you'd compute the gradient of the log-probabilitiy of the taken action with respect to the network parameters. In practice, this is typically done using automatic differentiation libaries, which can automatically compute these gradients. (Confused about this step). \n",
        "\n",
        "3. Scale the gradient by the \n",
        "\n",
        "\n",
        "\n",
        "## **Follow up question: What is exact paramterization this distribution that the neural network is computing parameters for? Do we get to choose this distribution?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L1gfYW2G0xox"
      }
    }
  ]
}